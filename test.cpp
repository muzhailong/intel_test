#include<iostream>
#include<omp.h>
#include "layers.h"
#include "component.h"
#include<memory>
#include<random>
#include<ctime>
#include<chrono>
#include<cmath>
#include<assert.h>
using namespace std;

void print(shared_ptr<Tensor>input,bool only_shape=false){
    float*buff=(float*)input->data;
    if(only_shape){
        cout<<"(";
        for(int t : input->dim.shape()){
            cout<<t<<",";
        }
        cout<<")"<<endl;
    }else{
        for(int i=0;i<input->dim.elements_size();++i){
            cout<<buff[i]<<",";
        }
    }
    cout<<endl;
}

bool equal(float*f1,float*f2,size_t n){
    float sm=.0;
    for(int i=0;i<n;++i){
        sm+=(f1[i]-f2[i])*(f1[i]-f2[i]);
    }
    sm/=n;
    if(sm<1e-3){
        return true;
    }
    return false;
}

void print_arr(float*arr,size_t n){
  for(int i=0;i<n;++i){
    cout<<arr[i]<<",";
  }
  cout<<endl;
}


void test(){
    float*buff=(float*)new float[2][5][5]
        {{{-0.82276605,  0.04053673,  0.95625502,  1.30901416,
         -1.74549885},
        { 0.13586256,  0.638937  , -0.53637478,  0.86319307,
          1.1404235 },
        {-0.44353018,  0.40528222,  0.86494899, -0.85611806,
         -2.48314196},
        {-0.63859645,  0.65135325, -0.29483573,  0.34516846,
          0.24779054},
        { 1.8104596 ,  0.52494297, -0.69277414,  1.05455806,
          0.78931865}},
       {{-1.69977458,  0.82656739, -0.03580635,  1.71742685,
          0.26220926},
        { 0.51832098,  0.82932258, -0.37122151, -0.20678568,
         -0.30792185},
        {-1.95841702,  0.59415733,  0.70449739, -0.13277688,
         -1.98120858},
        { 1.08267187,  0.36669053, -0.90616609, -0.63362229,
         -1.44015525},
        { 0.61930017,  0.3017664 , -1.05685089,  0.77402361,
         -0.27387332}}}; //(2,5,5)

        auto input=make_shared<Tensor>((float*)buff,Dim(2,5,5));
        float*k1=
            (float*)new float[3][3]
            {{-0.6430425 , -0.83873835,  0.55170773},
            { 0.14508247,  0.50770611,  0.56346479},
            {-0.23526905,  1.15214414,  1.18860292}};
        auto k1_t=make_shared<Tensor>(k1,Dim(3,3));
        float*relu_result=
        (float*)new float[2][5][5]{{{0.        , 0.04053673, 0.95625502, 1.30901416, 0.        },
        {0.13586256, 0.638937  , 0.        , 0.86319307, 1.1404235 },
        {0.        , 0.40528222, 0.86494899, 0.        , 0.        },
        {0.        , 0.65135325, 0.        , 0.34516846, 0.24779054},
        {1.8104596 , 0.52494297, 0.        , 1.05455806, 0.78931865}},
       {{0.        , 0.82656739, 0.        , 1.71742685, 0.26220926},
        {0.51832098, 0.82932258, 0.        , 0.        , 0.        },
        {0.        , 0.59415733, 0.70449739, 0.        , 0.        },
        {1.08267187, 0.36669053, 0.        , 0.        , 0.        },
        {0.61930017, 0.3017664 , 0.        , 0.77402361, 0.        }}};
        float*bn_result=
        (float*) new float[2][5][5]
        {{{ 0.        , -0.99999676,  0.99999781, -0.99998801,
         -0.99997091},
        {-0.99998633, -0.99994483,  0.        ,  0.99999732,
          0.99999846},
        {0.        , -0.99994394,  0.99992232,  0.        ,
          0.        },
        {-0.99999829,  0.99997532,  0.        ,  0.99998321,
          0.99996743},
        { 0.99999859,  0.99995985,  0.        ,  0.99997459,
          0.99999679}},

       {{ 0.        ,  0.99999676, -0.99999781,  0.99998801,
          0.99997091},
        { 0.99998633,  0.99994483,  0.        , -0.99999732,
         -0.99999846},
        { 0.        ,  0.99994394, -0.99992232,  0.        ,
          0.        },
        { 0.99999829, -0.99997532,  0.        , -0.99998321,
         -0.99996743},
        {-0.99999859, -0.99995985,  0.        , -0.99997459,
         -0.99999679}}};
         float*conv_with_no_padding_result=
         (float*)new float[2][1][3][3]{{
             {{0.08601273402188152, 0.11758912274771822, 0.05332371314219744},
          {0.32498372347182714, 0.2789583699198144, 0.2443034423205988}, 
          {0.2966440805242223, 0.1629089540750505, 0.3076517159677369}}}, 
          {{{-0.08601273402188152, -0.11758912274771822, -0.05332371314219744}, 
          {-0.32498372347182714, -0.2789583699198144, -0.2443034423205988}, 
          {-0.2966440805242223, -0.1629089540750505, -0.3076517159677369}}}};
        float* conv_with_padding1_result=(float*)new float[2][1][5][5]
        {{{{-0.32268096659339895, -0.09567285015064482, 0.13589119654270443, 0.15718620807351094, 0.029344652769848188}, 
        {-0.31237501141025237, 0.08601273402188152, 0.11758912274771822, 0.05332371314219744, 0.23717068337037717}, 
        {-0.026661354950582132, 0.32498372347182714, 0.2789583699198144, 0.2443034423205988, -0.06277070223959234}, 
        {0.2049740615190333, 0.2966440805242223, 0.1629089540750505, 0.3076517159677369, 0.17440521976457746}, 
        {0.27350873229263456, 0.05078797127702264, 0.06857766160162544, 0.08712463152909992, -0.09210660798783792}}}, 
        {{{0.32268096659339895, 0.09567285015064482, -0.13589119654270443, -0.15718620807351094, -0.029344652769848188}, 
        {0.31237501141025237, -0.08601273402188152, -0.11758912274771822, -0.05332371314219744, -0.23717068337037717}, 
        {0.026661354950582132, -0.32498372347182714, -0.2789583699198144, -0.2443034423205988, 0.06277070223959234}, 
        {-0.2049740615190333, -0.2966440805242223, -0.1629089540750505, -0.3076517159677369, -0.17440521976457746}, 
        {-0.27350873229263456, -0.05078797127702264, -0.06857766160162544, -0.08712463152909992, 0.09210660798783792}}}};



        Relu relu_obj;
        BatchNorm2D bn(Dim(5,5));
        Conv2D conv2d(vector<shared_ptr<Tensor>>{k1_t});
        Conv2D conv2d_with_padding1(vector<shared_ptr<Tensor>>{k1_t},1,1);

        auto h1=relu_obj.forward(input);
        assert(equal(relu_result,(float*)h1->data,h1->dim.elements_size()));
        cout<<"relu pass!!!!"<<endl;

        auto h2=bn.forward(h1);
        assert(equal(bn_result,(float*)h2->data,h2->dim.elements_size()));
        cout<<"bn pass!!!!"<<endl;

        auto h3=conv2d.forward(h2);
        assert(equal(conv_with_no_padding_result,(float*)h3->data,h3->dim.elements_size()));
        cout<<"conv2d pass!!!"<<endl;

        auto h32=conv2d_with_padding1.forward(h2);
        assert(equal(conv_with_padding1_result,(float*)h32->data,h32->dim.elements_size()));
        cout<<"conv2d with padding pass!!!"<<endl;
}


int main(){
    test();
    return 0;
}